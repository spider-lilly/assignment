{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# 1) Install dependencies\n",
        "# ============================================================================\n",
        "!pip install -q dspy-ai beautifulsoup4 requests lxml pandas python-dotenv\n"
      ],
      "metadata": {
        "id": "qhzDUjrviLCa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7034207a-6745-49c4-8824-9f3e351f3b96"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/261.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m261.7/261.7 kB\u001b[0m \u001b[31m22.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/52.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.3/52.3 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/45.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.3/10.3 MB\u001b[0m \u001b[31m84.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m400.9/400.9 kB\u001b[0m \u001b[31m26.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.1/278.1 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import dspy\n",
        "import requests\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "from typing import List, Dict\n",
        "from pydantic import BaseModel, Field\n",
        "from urllib.parse import urlparse\n",
        "import time\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()"
      ],
      "metadata": {
        "id": "2ludElJ2PxeW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# SECTION 2: CONFIGURE DSPY WITH API KEY\n",
        "# ============================================================================\n",
        "API_KEY = os.getenv("APIKEY")n",
        "main_lm = dspy.LM(\n",
        "    \"openai/LongCat-Flash-Chat\",\n",
        "    api_key=API_KEY,\n",
        "    api_base=\"https://api.longcat.chat/openai/v1\"\n",
        ")\n",
        "dspy.settings.configure(lm=main_lm, adapter=dspy.XMLAdapter())\n",
        "print(\"DSPy configured successfully!\")"
      ],
      "metadata": {
        "id": "xWasx1qNQTEU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "23607170-5701-4628-eca8-0a96f181e019"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DSPy configured successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create output directory\n",
        "os.makedirs('output', exist_ok=True)\n",
        "print(\"Created 'output' directory\")\n",
        "\n",
        "# ============================================================================\n",
        "# SECTION 3: ENTITY + ATTRIBUTE EXTRACTION\n",
        "# ============================================================================\n",
        "\n",
        "class EntityWithAttr(BaseModel):\n",
        "    entity: str = Field(description=\"the named entity\")\n",
        "    attr_type: str = Field(description=\"semantic type (e.g. Drug, Disease, Concept, Process)\")\n",
        "\n",
        "class ExtractEntities(dspy.Signature):\n",
        "    \"\"\"From the paragraph extract all relevant entities and their semantic attribute types.\"\"\"\n",
        "    paragraph: str = dspy.InputField(desc=\"input paragraph\")\n",
        "    entities: List[EntityWithAttr] = dspy.OutputField(desc=\"list of entities and their types\")\n",
        "\n",
        "extractor = dspy.Predict(ExtractEntities)\n",
        "\n",
        "# ============================================================================\n",
        "# SECTION 4: DEDUPLICATOR (recursive batching + confidence loop)\n",
        "# ============================================================================\n",
        "\n",
        "class DeduplicateEntities(dspy.Signature):\n",
        "    \"\"\"Given a list of (entity, attr_type) decide which ones are duplicates.\n",
        "    Return a deduplicated list and a confidence that the remaining items are ALL distinct.\"\"\"\n",
        "    items: List[EntityWithAttr] = dspy.InputField(desc=\"batch of entities to deduplicate\")\n",
        "    deduplicated: List[EntityWithAttr] = dspy.OutputField(desc=\"deduplicated list\")\n",
        "    confidence: float = dspy.OutputField(\n",
        "        desc=\"confidence (0-1) that every item is semantically distinct\"\n",
        "    )\n",
        "\n",
        "dedup_predictor = dspy.ChainOfThought(DeduplicateEntities)\n",
        "\n",
        "def deduplicate_with_lm(\n",
        "    items: List[EntityWithAttr],\n",
        "    batch_size: int = 10,\n",
        "    target_confidence: float = 0.9,\n",
        "    max_attempts: int = 3\n",
        ") -> List[EntityWithAttr]:\n",
        "    \"\"\"\n",
        "    Recursively deduplicate using the LM.\n",
        "    Works by:\n",
        "      1. splitting into batches of `batch_size`\n",
        "      2. for each batch asking the LM for duplicates + confidence\n",
        "      3. rerunning the batch until confidence >= target_confidence\n",
        "      4. concatenating results from all batches\n",
        "    \"\"\"\n",
        "    if not items:\n",
        "        return []\n",
        "    # helper to process one batch\n",
        "    def _process_batch(batch: List[EntityWithAttr]) -> List[EntityWithAttr]:\n",
        "        for attempt in range(max_attempts):\n",
        "            try:   #using try except block instead for better handling\n",
        "                pred = dedup_predictor(items=batch)\n",
        "                if pred.confidence >= target_confidence:\n",
        "                    return pred.deduplicated\n",
        "            except Exception as e:\n",
        "                print(f\"  Warning: Dedup attempt {attempt + 1} failed: {e}\")\n",
        "                if attempt == max_attempts - 1:\n",
        "                    return batch  # Return original if all attempts fail\n",
        "        return batch\n",
        "\n",
        "    # split into batches and process\n",
        "    results = []\n",
        "    for i in range(0, len(items), batch_size):\n",
        "        batch = items[i : i + batch_size]\n",
        "        results.extend(_process_batch(batch))\n",
        "    return results\n",
        "\n",
        "# ============================================================================\n",
        "# SECTION 5: RELATION EXTRACTION\n",
        "# ============================================================================\n",
        "\n",
        "class Relation(BaseModel):\n",
        "    subj: str = Field(description=\"subject entity\")\n",
        "    pred: str = Field(description=\"predicate/relation phrase\")\n",
        "    obj: str = Field(description=\"object entity\")\n",
        "\n",
        "class ExtractRelations(dspy.Signature):\n",
        "    \"\"\"Given the original paragraph and a list of unique entities,\n",
        "    extract all factual (subject, predicate, object) triples that are explicitly stated or clearly implied.\"\"\"\n",
        "    paragraph: str = dspy.InputField(desc=\"original paragraph\")\n",
        "    entities: List[str] = dspy.InputField(desc=\"list of deduplicated entities\")\n",
        "    relations: List[Relation] = dspy.OutputField(desc=\"subject-predicate-object triples\")\n",
        "\n",
        "rel_predictor = dspy.ChainOfThought(ExtractRelations)\n",
        "\n",
        "# ============================================================================\n",
        "# SECTION 6: MERMAID SERIALISER  (revised)\n",
        "# ============================================================================\n",
        "\n",
        "def triples_to_mermaid(\n",
        "    triples: List[Relation],\n",
        "    entity_list: List[str],\n",
        "    max_label_len: int = 40\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    Convert triples to a VALID Mermaid flowchart LR diagram.\n",
        "    \"\"\"\n",
        "    entity_set = {e.strip().lower() for e in entity_list}\n",
        "    lines = [\"flowchart LR\"]\n",
        "\n",
        "    def _make_id(s: str) -> str:\n",
        "        # Create valid Mermaid node ID (no spaces or special chars)\n",
        "        return s.strip().replace(\" \", \"_\").replace(\"(\", \"\").replace(\")\", \"\").replace(\"-\", \"_\").replace(\"/\", \"_\")\n",
        "\n",
        "    for t in triples:\n",
        "        subj_norm = t.subj.strip().lower()\n",
        "        obj_norm = t.obj.strip().lower()\n",
        "\n",
        "        # Only include relations where both entities are in our list\n",
        "        if subj_norm in entity_set and obj_norm in entity_set:\n",
        "            src, dst, lbl = t.subj, t.obj, t.pred\n",
        "\n",
        "            # Sanitize label\n",
        "            lbl = lbl.strip()\n",
        "            if len(lbl) > max_label_len:\n",
        "                lbl = lbl[:max_label_len - 3] + \"...\"\n",
        "\n",
        "            # Generate Mermaid edge\n",
        "            src_id, dst_id = _make_id(src), _make_id(dst)\n",
        "            lines.append(f'    {src_id}[\"{src}\"] -->|{lbl}| {dst_id}[\"{dst}\"]')\n",
        "\n",
        "    return \"\\n\".join(lines)\n",
        "\n",
        "def create_error_mermaid(url: str, error_msg: str) -> str:\n",
        "    \"\"\"Create a Mermaid diagram showing error information\"\"\"\n",
        "    return f\"\"\"flowchart TD\n",
        "    Error[\\\"ERROR: Processing Failed\\\"]\n",
        "    URL[\\\"URL: {url[:50]}...\\\"]\n",
        "    Message[\\\"Error: {error_msg[:100]}...\\\"]\n",
        "\n",
        "    Error --> URL\n",
        "    Error --> Message\"\"\"\n",
        "\n",
        "# ============================================================================\n",
        "# SECTION 7: WEB SCRAPING FUNCTIONS\n",
        "# ============================================================================\n",
        "#setting a limit to avoid api error\n",
        "def scrape_url(url: str, max_chars: int = 15000) -> str:\n",
        "    \"\"\"\n",
        "    Scrape text content from a URL.\n",
        "    \"\"\"\n",
        "    try:\n",
        "\n",
        "        headers = {\n",
        "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
        "        }\n",
        "        response = requests.get(url, headers=headers, timeout=30)\n",
        "        response.raise_for_status()\n",
        "\n",
        "        # Parse HTML content\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "        # Remove script and style elements\n",
        "        for script in soup([\"script\", \"style\", \"nav\", \"footer\", \"header\"]):\n",
        "            script.decompose()\n",
        "\n",
        "        # Get text\n",
        "        text = soup.get_text(separator=' ', strip=True)\n",
        "\n",
        "        # Clean up whitespace\n",
        "        lines = (line.strip() for line in text.splitlines())\n",
        "        chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
        "        text = ' '.join(chunk for chunk in chunks if chunk)\n",
        "\n",
        "        # Limit length to avoid api error\n",
        "        if len(text) > max_chars:\n",
        "            text = text[:max_chars]\n",
        "\n",
        "        return text\n",
        "\n",
        "    except Exception as e:\n",
        "        raise Exception(f\"Scraping failed: {str(e)}\")\n",
        "\n",
        "# ============================================================================\n",
        "# SECTION 8: MAIN PROCESSING PIPELINE\n",
        "# ============================================================================\n",
        "\n",
        "def process_url(url: str, url_index: int) -> Dict:\n",
        "    \"\"\"\n",
        "    Complete pipeline for processing a single URL:\n",
        "    1. Scrape content\n",
        "    2. Extract entities\n",
        "    3. Deduplicate entities\n",
        "    4. Extract relations\n",
        "    5. Generate Mermaid diagram\n",
        "    \"\"\"\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"Processing URL {url_index}: {url}\")\n",
        "    print(f\"{'='*80}\")\n",
        "\n",
        "    error_message = None\n",
        "\n",
        "    try:\n",
        "        # Step 1: Scrape content\n",
        "        print(\"Scraping content...\")\n",
        "        content = scrape_url(url)\n",
        "        print(f\"Scraped {len(content)} characters\")\n",
        "\n",
        "        # Step 2: Extract entities\n",
        "        print(\"Extracting entities...\")\n",
        "        extracted = extractor(paragraph=content)\n",
        "        print(f\"Extracted {len(extracted.entities)} entities\")\n",
        "\n",
        "        # Step 3: Deduplicate entities\n",
        "        print(\"Deduplicating entities...\")\n",
        "        unique = deduplicate_with_lm(extracted.entities, batch_size=10, target_confidence=0.85)\n",
        "        print(f\"Deduplicated to {len(unique)} unique entities\")\n",
        "\n",
        "        entity_strings = [e.entity for e in unique]\n",
        "\n",
        "        # Step 4: Extract relations (split content if too long)\n",
        "        print(\"Extracting relations...\")\n",
        "        chunk_size = 8000\n",
        "        all_relations = []\n",
        "\n",
        "        for i in range(0, len(content), chunk_size):\n",
        "            chunk = content[i:i + chunk_size]\n",
        "            rel_out = rel_predictor(paragraph=chunk, entities=entity_strings)\n",
        "            all_relations.extend(rel_out.relations)\n",
        "        # Step 5: Generate Mermaid diagram\n",
        "        print(\"Generating Mermaid diagram...\")\n",
        "        mermaid_code = triples_to_mermaid(all_relations, entity_strings)\n",
        "\n",
        "        return {\n",
        "            'url': url,\n",
        "            'entities': unique,\n",
        "            'relations': all_relations,\n",
        "            'mermaid': mermaid_code,\n",
        "            'error': None\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        error_message = str(e)\n",
        "        print(f\"Processing failed: {error_message}\")\n",
        "\n",
        "        # Return error result with error mermaid diagram\n",
        "        return {\n",
        "            'url': url,\n",
        "            'entities': [],\n",
        "            'relations': [],\n",
        "            'mermaid': create_error_mermaid(url, error_message),\n",
        "            'error': error_message\n",
        "        }\n",
        "\n",
        "# ============================================================================\n",
        "# SECTION 9: PROCESS ALL URLs\n",
        "# ============================================================================\n",
        "\n",
        "# Define the 10 URLs from the assignment\n",
        "URLS = [\n",
        "    \"https://en.wikipedia.org/wiki/Sustainable_agriculture\",\n",
        "    \"https://www.nature.com/articles/d41586-025-03353-5\",\n",
        "    \"https://www.sciencedirect.com/science/article/pii/S1043661820315152\",\n",
        "    \"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10457221/\",\n",
        "    \"https://www.fao.org/3/y4671e/y4671e06.htm\",\n",
        "    \"https://www.medscape.com/viewarticle/time-reconsider-tramadol-chronic-pain-2025a1000ria\",\n",
        "    \"https://www.sciencedirect.com/science/article/pii/S0378378220307088\",\n",
        "    \"https://www.frontiersin.org/news/2025/09/01/rectangle-telescope-finding-habitable-planets\",\n",
        "    \"https://www.medscape.com/viewarticle/second-dose-boosts-shingles-protection-adults-aged-65-years-2025a1000ro7\",\n",
        "    \"https://www.theguardian.com/global-development/2025/oct/13/astro-ambassadors-stargazers-himalayas-hanle-ladakh-india\"\n",
        "]\n",
        "\n",
        "\n",
        "# Process all URLs\n",
        "print(\"Starting processing of 10 URLs...\")\n",
        "print(\"This may take 15-30 minutes depending on API rate limits.\\n\")\n",
        "\n",
        "results = []\n",
        "for i, url in enumerate(URLS, 1):\n",
        "    result = process_url(url, i)\n",
        "    results.append(result)\n",
        "\n",
        "    # Rate limiting: wait between requests\n",
        "    if i < len(URLS):\n",
        "        print(\"\\nWaiting 5 seconds before next URL...\")\n",
        "        time.sleep(5)\n",
        "\n",
        "print(f\"\\n\\n{'='*80}\")\n",
        "successful = sum(1 for r in results if r['error'] is None)\n",
        "print(f\"Processing complete! Successfully processed {successful}/{len(URLS)} URLs\")\n",
        "print(f\"{'='*80}\\n\")\n",
        "\n",
        "# ============================================================================\n",
        "# SECTION 10: SAVE MERMAID DIAGRAMS\n",
        "# ============================================================================\n",
        "\n",
        "print(\"Saving Mermaid diagrams to output/ directory...\")\n",
        "for i, result in enumerate(results, 1):\n",
        "    filename = f\"output/mermaid_{i}.md\"\n",
        "    with open(filename, 'w', encoding='utf-8') as f:\n",
        "        f.write(f\"# Knowledge Graph for URL {i}\\n\\n\")\n",
        "        f.write(f\"**Source:** {result['url']}\\n\\n\")\n",
        "\n",
        "        if result['error']:\n",
        "            f.write(f\"**Status:**  Processing Failed\\n\\n\")\n",
        "            f.write(f\"**Error Message:** {result['error']}\\n\\n\")\n",
        "        else:\n",
        "            f.write(f\"**Status:**  Successfully Processed\\n\\n\")\n",
        "            f.write(f\"**Entities Extracted:** {len(result['entities'])}\\n\\n\")\n",
        "            f.write(f\"**Relations Found:** {len(result['relations'])}\\n\\n\")\n",
        "\n",
        "        f.write(\"```mermaid\\n\")\n",
        "        f.write(result['mermaid'])\n",
        "        f.write(\"\\n```\\n\")\n",
        "    print(f\"Saved {filename}\")\n",
        "\n",
        "# ============================================================================\n",
        "# SECTION 11: CREATE CSV FILE\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\nCreating tags.csv in output/ directory...\")\n",
        "\n",
        "# Build CSV data (only from successful extractions)\n",
        "csv_data = []\n",
        "for result in results:\n",
        "    if result['error'] is None:  # Only include successful extractions\n",
        "        url = result['url']\n",
        "        for entity in result['entities']:\n",
        "            csv_data.append({\n",
        "                'link': url,\n",
        "                'tag': entity.entity,\n",
        "                'tag_type': entity.attr_type\n",
        "            })\n",
        "\n",
        "# Create DataFrame and save\n",
        "if csv_data:\n",
        "    df = pd.DataFrame(csv_data)\n",
        "    df.to_csv('output/tags.csv', index=False, encoding='utf-8')\n",
        "\n",
        "    print(f\"Saved output/tags.csv with {len(csv_data)} rows\")\n",
        "else:\n",
        "    print(\" No successful extractions - CSV not created\")\n",
        "\n",
        "print(\"\\nAssignment complete! Ready for submission.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wKQP01enrZcj",
        "outputId": "22036052-df08-460a-891e-ee2719ab0c72"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created 'output' directory\n",
            "Starting processing of 10 URLs...\n",
            "This may take 15-30 minutes depending on API rate limits.\n",
            "\n",
            "\n",
            "================================================================================\n",
            "Processing URL 1: https://en.wikipedia.org/wiki/Sustainable_agriculture\n",
            "================================================================================\n",
            "Scraping content...\n",
            "Scraped 15000 characters\n",
            "Extracting entities...\n",
            "Extracted 89 entities\n",
            "Deduplicating entities...\n",
            "Deduplicated to 89 unique entities\n",
            "Extracting relations...\n",
            "Generating Mermaid diagram...\n",
            "\n",
            "Waiting 5 seconds before next URL...\n",
            "\n",
            "================================================================================\n",
            "Processing URL 2: https://www.nature.com/articles/d41586-025-03353-5\n",
            "================================================================================\n",
            "Scraping content...\n",
            "Scraped 5910 characters\n",
            "Extracting entities...\n",
            "Extracted 28 entities\n",
            "Deduplicating entities...\n",
            "Deduplicated to 27 unique entities\n",
            "Extracting relations...\n",
            "Generating Mermaid diagram...\n",
            "\n",
            "Waiting 5 seconds before next URL...\n",
            "\n",
            "================================================================================\n",
            "Processing URL 3: https://www.sciencedirect.com/science/article/pii/S1043661820315152\n",
            "================================================================================\n",
            "Scraping content...\n",
            "Processing failed: Scraping failed: 403 Client Error: Forbidden for url: https://www.sciencedirect.com/science/article/pii/S1043661820315152\n",
            "\n",
            "Waiting 5 seconds before next URL...\n",
            "\n",
            "================================================================================\n",
            "Processing URL 4: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10457221/\n",
            "================================================================================\n",
            "Scraping content...\n",
            "Scraped 15000 characters\n",
            "Extracting entities...\n",
            "Extracted 43 entities\n",
            "Deduplicating entities...\n",
            "Deduplicated to 42 unique entities\n",
            "Extracting relations...\n",
            "Generating Mermaid diagram...\n",
            "\n",
            "Waiting 5 seconds before next URL...\n",
            "\n",
            "================================================================================\n",
            "Processing URL 5: https://www.fao.org/3/y4671e/y4671e06.htm\n",
            "================================================================================\n",
            "Scraping content...\n",
            "Scraped 15000 characters\n",
            "Extracting entities...\n",
            "Extracted 75 entities\n",
            "Deduplicating entities...\n",
            "Deduplicated to 74 unique entities\n",
            "Extracting relations...\n",
            "Generating Mermaid diagram...\n",
            "\n",
            "Waiting 5 seconds before next URL...\n",
            "\n",
            "================================================================================\n",
            "Processing URL 6: https://www.medscape.com/viewarticle/time-reconsider-tramadol-chronic-pain-2025a1000ria\n",
            "================================================================================\n",
            "Scraping content...\n",
            "Scraped 8593 characters\n",
            "Extracting entities...\n",
            "Extracted 44 entities\n",
            "Deduplicating entities...\n",
            "Deduplicated to 44 unique entities\n",
            "Extracting relations...\n",
            "Generating Mermaid diagram...\n",
            "\n",
            "Waiting 5 seconds before next URL...\n",
            "\n",
            "================================================================================\n",
            "Processing URL 7: https://www.sciencedirect.com/science/article/pii/S0378378220307088\n",
            "================================================================================\n",
            "Scraping content...\n",
            "Processing failed: Scraping failed: 403 Client Error: Forbidden for url: https://www.sciencedirect.com/science/article/pii/S0378378220307088\n",
            "\n",
            "Waiting 5 seconds before next URL...\n",
            "\n",
            "================================================================================\n",
            "Processing URL 8: https://www.frontiersin.org/news/2025/09/01/rectangle-telescope-finding-habitable-planets\n",
            "================================================================================\n",
            "Scraping content...\n",
            "Scraped 9236 characters\n",
            "Extracting entities...\n",
            "Extracted 50 entities\n",
            "Deduplicating entities...\n",
            "Deduplicated to 48 unique entities\n",
            "Extracting relations...\n",
            "Generating Mermaid diagram...\n",
            "\n",
            "Waiting 5 seconds before next URL...\n",
            "\n",
            "================================================================================\n",
            "Processing URL 9: https://www.medscape.com/viewarticle/second-dose-boosts-shingles-protection-adults-aged-65-years-2025a1000ro7\n",
            "================================================================================\n",
            "Scraping content...\n",
            "Scraped 6144 characters\n",
            "Extracting entities...\n",
            "Extracted 42 entities\n",
            "Deduplicating entities...\n",
            "Deduplicated to 40 unique entities\n",
            "Extracting relations...\n",
            "Generating Mermaid diagram...\n",
            "\n",
            "Waiting 5 seconds before next URL...\n",
            "\n",
            "================================================================================\n",
            "Processing URL 10: https://www.theguardian.com/global-development/2025/oct/13/astro-ambassadors-stargazers-himalayas-hanle-ladakh-india\n",
            "================================================================================\n",
            "Scraping content...\n",
            "Scraped 9067 characters\n",
            "Extracting entities...\n",
            "Extracted 33 entities\n",
            "Deduplicating entities...\n",
            "Deduplicated to 33 unique entities\n",
            "Extracting relations...\n",
            "Generating Mermaid diagram...\n",
            "\n",
            "\n",
            "================================================================================\n",
            "Processing complete! Successfully processed 8/10 URLs\n",
            "================================================================================\n",
            "\n",
            "Saving Mermaid diagrams to output/ directory...\n",
            "Saved output/mermaid_1.md\n",
            "Saved output/mermaid_2.md\n",
            "Saved output/mermaid_3.md\n",
            "Saved output/mermaid_4.md\n",
            "Saved output/mermaid_5.md\n",
            "Saved output/mermaid_6.md\n",
            "Saved output/mermaid_7.md\n",
            "Saved output/mermaid_8.md\n",
            "Saved output/mermaid_9.md\n",
            "Saved output/mermaid_10.md\n",
            "\n",
            "Creating tags.csv in output/ directory...\n",
            "Saved output/tags.csv with 397 rows\n",
            "\n",
            "Assignment complete! Ready for submission.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "import shutil\n",
        "\n",
        "# Zip the output directory\n",
        "shutil.make_archive('output', 'zip', 'output')\n",
        "\n",
        "# Download the zip file\n",
        "files.download('output.zip')"
      ],
      "metadata": {
        "id": "BuBZNunjPUj1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "f74ccf37-c55a-4194-c127-cbbd974515d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_9dcaab35-4bfa-45b0-a25c-d185fe625805\", \"output.zip\", 14079)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}
